{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;66;03m# Append the fight details to the all_fights list\u001b[39;00m\n\u001b[1;32m     66\u001b[0m                 all_fights\u001b[38;5;241m.\u001b[39mappend(fight_details)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 1-second delay between event scrapes\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Convert the all_fights list into a DataFrame\u001b[39;00m\n\u001b[1;32m     72\u001b[0m all_fights_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_fights)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Base URL for UFC events\n",
    "base_url = \"http://ufcstats.com/statistics/events/completed?page=\"\n",
    "\n",
    "# Function to get the HTML of a given page number\n",
    "def get_page_html(page_number):\n",
    "    url = base_url + str(page_number)\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Scrape all pages of completed UFC events\n",
    "all_events = []\n",
    "page_number = 1\n",
    "has_more_pages = True\n",
    "\n",
    "while has_more_pages:\n",
    "    soup = get_page_html(page_number)\n",
    "    \n",
    "    # Extract event details from the current page\n",
    "    event_list = soup.find_all(\"a\", class_=\"b-link b-link_style_black\")\n",
    "    if not event_list:\n",
    "        has_more_pages = False  # Exit if no more events are found on the page\n",
    "    else:\n",
    "        for event in event_list:\n",
    "            event_name = event.text.strip()\n",
    "            event_url = event['href']\n",
    "            all_events.append({\"event_name\": event_name, \"event_url\": event_url})\n",
    "        \n",
    "        page_number += 1  # Move to the next page\n",
    "        time.sleep(1)  # Delay to avoid overloading the server\n",
    "\n",
    "# Convert to DataFrame\n",
    "events_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Scrape fights for each event and store them all in one list\n",
    "all_fights = []\n",
    "for index, row in events_df.iterrows():\n",
    "    event_name = row['event_name']\n",
    "    event_url = row['event_url']\n",
    "    \n",
    "    # Request the event page\n",
    "    event_response = requests.get(event_url)\n",
    "    event_soup = BeautifulSoup(event_response.content, \"html.parser\")\n",
    "    \n",
    "    # Scrape fight details (fight table)\n",
    "    fight_table = event_soup.find(\"tbody\")\n",
    "    \n",
    "    # Some pages might not have fights listed, so we check first\n",
    "    if fight_table:\n",
    "        for fight_row in fight_table.find_all(\"tr\"):\n",
    "            fight_data = fight_row.find_all(\"td\")\n",
    "            \n",
    "            # Ensure the correct number of columns are present (should be 7 for each fight)\n",
    "            if len(fight_data) >= 7:\n",
    "                # Collect fight details\n",
    "                fight_details = {\n",
    "                    \"event\": event_name,\n",
    "                    \"fighter_1\": fight_data[1].find_all(\"p\")[0].text.strip(),  # First fighter name\n",
    "                    \"fighter_2\": fight_data[1].find_all(\"p\")[1].text.strip(),  # Second fighter name\n",
    "                    \"result\": fight_data[0].text.strip(),  # Win/Loss\n",
    "                    \"method\": fight_data[7].text.strip(),  # Method of victory\n",
    "                    \"round\": fight_data[8].text.strip(),  # Round number\n",
    "                    \"time\": fight_data[9].text.strip()  # Time of fight\n",
    "                }\n",
    "                \n",
    "                # Append the fight details to the all_fights list\n",
    "                all_fights.append(fight_details)\n",
    "    \n",
    "    \n",
    "    time.sleep(.1)  # 1-second delay between event scrapes\n",
    "\n",
    "# Convert the all_fights list into a DataFrame\n",
    "all_fights_df = pd.DataFrame(all_fights)\n",
    "\n",
    "# Save the entire dataset to one CSV file\n",
    "all_fights_df.to_csv(\"ufcfights.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
